{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoGPT\n",
    "In this assignment you will learn to implement a GPT model from scratch. This includes implementing the Transformer architecture (with causal input mask), the GPT model, a byte-level BPE tokenizer, the embedding layer, and the positional encoding layer.\n",
    "We will train a GPT-2 Model ([Original Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\cxcyh\\.conda\\envs\\cs182\\lib\\site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from grader import *\n",
    "import typing\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenizer\n",
    "\n",
    "To transform text into a format that can be used by a neural net, we need to first tokenize it (That is, transform the text corpus into indexes of our dictionary). The GPT-2 model uses a byte-level BPE tokenizer. However, we will only try to implement a unicode tokenizer in this section for simplicity.\n",
    "\n",
    "Before we start, please first take a look at [this tutorial video by Huggingface](https://youtu.be/HEikzVL-lZU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tutorial video above, you see that a BPE tokenizer starts with a base dictionary set of characters. Those \"base set of characters\" are usually represented by unicode characters. Unicode characters are the standard way to represent all possible human languages plus our favorite emojis ðŸ˜€ðŸ˜™ in byte streams. There are multiple Unicode standards include UTF-8, UTF-16, UTF-32, etc. However, because unicode characters are not very memory efficients, this basically means that we need to start with a **huge** base dictionary size to begin with our tokenizer. And this is why the authors of GPT-2 chooses to instead use a byte-level BPE tokenizer. That is, we split characters into futher smaller fragments (1 byte, or 8-bits) and use those as our base dictionary. This way, we can reduce the base dictionary size from 100,000+ unicode characters to just 256.\n",
    "\n",
    "> As a side note, the UTF-8 standard is not a strict 8-bit-per-character standard and each character in a UTF-8 stream can take more than 8 bits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self, base_dictionary : typing.List[bytes] = [i.to_bytes(1,'big') for i in range(256)]) -> None:\n",
    "        \n",
    "        # dictionary holds all volcabulary items and the index of each item in this array will be the input idx to the model\n",
    "        self.dictionary_array : typing.List[bytes] = base_dictionary\n",
    "\n",
    "        # This is a dictionary that maps a combination of two vocab items to a later vocab item\n",
    "        self.combinations_to_index : typing.Dict[typing.Tuple[int, int], int] = {}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dictionary_array)\n",
    "    \n",
    "    def __getitem__(self, key: int) -> str:\n",
    "        return self.dictionary_array[key]\n",
    "    \n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self.dictionary_array\n",
    "    \n",
    "    def expand_dictionary(self, combination_vocab : typing.Tuple[int, int]) -> None:\n",
    "        \"\"\"\n",
    "        This function should expand the dictionary with one more vocabulary item, the item should be the concatenation of the two vocab items in combination_vocab\n",
    "        Parameters\n",
    "        ----------\n",
    "        combination_vocab : typing.Tuple[int, int]\n",
    "            The combination of two vocab items to expand the dictionary with\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # self.dictionary_array.append(self.dictionary_array[combination_vocab[0]] + self.dictionary_array[combination_vocab[1]])\n",
    "        # self.combinations_to_index[combination_vocab] = len(self.dictionary_array) - 1\n",
    "    \n",
    "\n",
    "    def find_combination_to_expand(corpus_of_text: typing.List[int]) -> typing.Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        This function should find the combination of two vocab items that occurs the most in the corpus of text and return it\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus_of_text : typing.List[int]\n",
    "            The corpus of text represented by a list of integers (with each integer representing a vocab in the dictionary) to expand the dictionary with\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # count_dict = {}\n",
    "        # for i in range(len(corpus_of_text) - 1):\n",
    "        #     if (corpus_of_text[i], corpus_of_text[i+1]) in count_dict:\n",
    "        #         count_dict[(corpus_of_text[i], corpus_of_text[i+1])] += 1\n",
    "        #     else:\n",
    "        #         count_dict[(corpus_of_text[i], corpus_of_text[i+1])] = 1\n",
    "        # return max(count_dict, key=count_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your implementation\n",
    "grade_dictionary_class_expand_dictionary(Dictionary(), n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
