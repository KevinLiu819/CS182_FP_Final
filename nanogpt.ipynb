{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoGPT\n",
    "In this assignment you will learn to implement a GPT model from scratch. This includes implementing the Transformer architecture (with causal input mask), the GPT model, a byte-level BPE tokenizer, the embedding layer, and the positional encoding layer.\n",
    "We will train a GPT-2 Model ([Original Paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\cxcyh\\.conda\\envs\\cs182\\lib\\site-packages (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from grader import *\n",
    "import typing\n",
    "import numpy as np\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup autograder submission\n",
    "submitter = AutograderSubmitter()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tokenizer\n",
    "\n",
    "To transform text into a format that can be used by a neural net, we need to first tokenize it (That is, transform the text corpus into indexes of our dictionary). The GPT-2 model uses a byte-level BPE tokenizer. \n",
    "\n",
    "Before we start, please first take a look at [this tutorial video by Huggingface](https://youtu.be/HEikzVL-lZU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tutorial video above, you see that a BPE tokenizer starts with a base dictionary set of characters. Those \"base set of characters\" are usually represented by unicode characters. Unicode characters are the standard way to represent all possible human languages plus our favorite emojis ðŸ˜€ðŸ˜™ in byte streams. There are multiple Unicode standards include UTF-8, UTF-16, UTF-32, etc. However, because unicode characters are not very memory efficients, this basically means that we need to start with a **huge** base dictionary size to begin with our tokenizer. And this is why the authors of GPT-2 chooses to instead use a byte-level BPE tokenizer. That is, we split characters into futher smaller fragments (1 byte, or 8-bits) and use those as our base dictionary. This way, we can reduce the base dictionary size from 100,000+ unicode characters to just 256.\n",
    "\n",
    "> As a side note, the UTF-8 standard is not a strict 8-bit-per-character standard and each character in a UTF-8 stream can take more than 8 bits. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a byte-level BPE tokenizer, we need to first create a base dictionary of 256 characters. This base dictionary will be constructed and passed into our `Dictionary` class via the `__init__` method. Then we need to be able to expand our vocabulary list, one at a time. We will do this by implementing both the `expand_dictionary` method and the `find_combinations_to_expand` method.\n",
    "\n",
    "Basically, think about our current dictionary as all possible alphabets and a space, we would first tokenize our text into a list of indexes in our dictionary. Then we will enumerate through the list of indexes and find the most frequent pair of indexes that appear next to each other. We will then combine those two indexes (of the most frequent pair) into a new index and add it to our dictionary. We will repeat this process until we reach our desired vocabulary size.\n",
    "\n",
    "There are two member attributes of the `Dictionary` class, `dictionary_array` and `combinations_to_index`. The `dictionary_array` attribute simply holds all the vocabularies and `combinations_to_index` is used later in `tokenize()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self, base_dictionary : typing.List[bytes] = [i.to_bytes(1,'big') for i in range(256)]) -> None:\n",
    "        \n",
    "        # dictionary holds all volcabulary items and the index of each item in this array will be the input idx to the model\n",
    "        self.dictionary_array : typing.List[bytes] = base_dictionary.copy()\n",
    "\n",
    "        # This is a dictionary that maps a combination of two vocab items to a later vocab item\n",
    "        self.combinations_to_index : typing.Dict[typing.Tuple[int, int], int] = {}\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dictionary_array)\n",
    "    \n",
    "    def __getitem__(self, key: int) -> str:\n",
    "        return self.dictionary_array[key]\n",
    "    \n",
    "    def __contains__(self, key: str) -> bool:\n",
    "        return key in self.dictionary_array\n",
    "    \n",
    "    def expand_dictionary(self, combination_vocab : typing.Tuple[int, int]) -> None:\n",
    "        \"\"\"\n",
    "        This function should expand the dictionary with one more vocabulary item, \n",
    "        the item should be the concatenation of the two vocab items in combination_vocab\n",
    "        You need to modify both the dictionary_array and combinations_to_index\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        combination_vocab : typing.Tuple[int, int]\n",
    "            The combination of two vocab items to expand the dictionary with\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # self.dictionary_array.append(self.dictionary_array[combination_vocab[0]] + self.dictionary_array[combination_vocab[1]])\n",
    "        # self.combinations_to_index[combination_vocab] = len(self.dictionary_array) - 1\n",
    "    \n",
    "\n",
    "    def find_combination_to_expand(self, corpus_of_text: typing.List[int]) -> typing.Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        This function should find the combination of two vocab items that occurs the most in the corpus of text and return it\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus_of_text : typing.List[int]\n",
    "            The corpus of text represented by a list of integers (with each integer representing a vocab in the dictionary) to expand the dictionary with\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # count_dict = {}\n",
    "        # for i in range(len(corpus_of_text) - 1):\n",
    "        #     if (corpus_of_text[i], corpus_of_text[i+1]) in count_dict:\n",
    "        #         count_dict[(corpus_of_text[i], corpus_of_text[i+1])] += 1\n",
    "        #     else:\n",
    "        #         count_dict[(corpus_of_text[i], corpus_of_text[i+1])] = 1\n",
    "        # return max(count_dict, key=count_dict.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your implementation\n",
    "grade_dictionary_class_expand_dictionary(Dictionary())\n",
    "grade_dictionary_class_find_combination_to_expand(Dictionary())\n",
    "submitter.submission_data[\"tokenizer_comb\"] = generate_dictionary_class_find_combination_to_expand_dat(\n",
    "    Dictionary()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now we're able to expand our vocabulary list. But how do we tokenize our text into a list of indexes? We will do this by implementing the `tokenize` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text : str, dictionary : Dictionary) -> typing.List[int]:\n",
    "    \"\"\"\n",
    "    This function should tokenize the text using the dictionary and return the tokenized text as a list of integers\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text to tokenize\n",
    "    \n",
    "    dictionary : Dictionary\n",
    "        The dictionary to use for tokenization\n",
    "    \"\"\"\n",
    "\n",
    "    text_bytestream = bytes(text, \"utf-8\") # convert text to bytestream\n",
    "    tokenized_text : typing.List[int] = [] # initialize tokenized text\n",
    "    for i in range(len(text_bytestream)):\n",
    "        tokenized_text.append(\n",
    "            dictionary.dictionary_array.index(text_bytestream[i:i+1])\n",
    "        )\n",
    "    \n",
    "    num_tokenized_last_pass = len(tokenized_text)\n",
    "    # We will sweep through the tokenized text and replace any combination of two vocab items with the later vocab item\n",
    "    while num_tokenized_last_pass > 0:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        # num_tokenized_last_pass = 0\n",
    "        # new_tokenized_text = []\n",
    "        # for i in range(len(tokenized_text) - 1):\n",
    "        #     if (tokenized_text[i], tokenized_text[i+1]) in dictionary.combinations_to_index:\n",
    "        #         new_tokenized_text.append(dictionary.combinations_to_index[(tokenized_text[i], tokenized_text[i+1])])\n",
    "        #         num_tokenized_last_pass += 1\n",
    "        #     else:\n",
    "        #         new_tokenized_text.append(tokenized_text[i])\n",
    "        #         if i == len(tokenized_text) - 2:\n",
    "        #             new_tokenized_text.append(tokenized_text[i+1])\n",
    "        \n",
    "        # tokenized_text = new_tokenized_text\n",
    "    \n",
    "    return tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_tokenizer(tokenize, Dictionary())\n",
    "submitter.submission_data[\"tokenizer\"] = generate_tokenizer_submission(tokenize, Dictionary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather your submissions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will generate a `submission.npz` file. Please submit this file to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitter.generate_submission_file(\"submission.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal use\n",
    "from grader_internal import Autograder\n",
    "autograder = Autograder()\n",
    "grade = autograder.grade(submitter.submission_data)\n",
    "grade_2 = autograder.grade(np.load(\"submission.npz\"))\n",
    "assert np.all(grade == grade_2)\n",
    "print(\"Grade:\", np.mean(grade) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs182",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
